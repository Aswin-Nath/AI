{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cfb29c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.groq import llm\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.runnables import RunnableLambda,RunnableBranch\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b792156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class validity_output(BaseModel):\n",
    "    question:str\n",
    "    type:Literal[\"valid\",\"ambiguous\",\"invalid\"]\n",
    "validity_parser=PydanticOutputParser(pydantic_object=validity_output)\n",
    "validity_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\":validity_parser.get_format_instructions()\n",
    "    },\n",
    "    template=\"\"\"\n",
    "\n",
    "You are an expert on general topics.\n",
    "\n",
    "Given the question:\n",
    "{question}\n",
    "\n",
    "- valid: factual, time-independent, and semantically correct\n",
    "- ambiguous: factual but time-dependent or missing context\n",
    "- invalid: semantically impossible or uses incorrect roles\n",
    "\n",
    "Return as per below instruction\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    ")\n",
    "validity_chain = prompt=validity_prompt | llm | validity_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c93742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rewrite_output(BaseModel):\n",
    "    question:str\n",
    "rewrite_parser=PydanticOutputParser(pydantic_object=rewrite_output)\n",
    "rewrite_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": rewrite_parser.get_format_instructions()\n",
    "    },\n",
    "    template=\"\"\"\n",
    "You are a query rewriter.\n",
    "\n",
    "Task:\n",
    "- Rewrite the given question to be clear and unambiguous\n",
    "- Preserve the original intent\n",
    "- DO NOT answer the question\n",
    "- DO NOT define or explain concepts\n",
    "- DO NOT create schemas or metadata\n",
    "\n",
    "Input question:\n",
    "{question}\n",
    "\n",
    "Output rules:\n",
    "- Return ONLY the rewritten question\n",
    "- Follow the JSON format exactly\n",
    "- No extra text\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "rewrite_chain=rewrite_prompt|llm|rewrite_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48290a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class intent_output(BaseModel):\n",
    "    question:str\n",
    "    intent: Literal[\"question\", \"definition\", \"comparison\", \"opinion\"]\n",
    "intent_parser=PydanticOutputParser(pydantic_object=intent_output)\n",
    "intent_prompt=PromptTemplate(input_variables=[\"question\"],template=\"\"\"\n",
    "classify this {question} based on these categories question,definition,comparison,opinion\n",
    "Choose the PRIMARY intent only.\n",
    "Ignore secondary requests.\n",
    "Return exactly one intent.\n",
    "return according to below format\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "partial_variables={\"format_instructions\":intent_parser.get_format_instructions()}\n",
    ")\n",
    "intent_chain=intent_prompt|llm|intent_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0bc949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class answer_output(BaseModel):\n",
    "    question:str\n",
    "    answer:str\n",
    "answer_parser=PydanticOutputParser(pydantic_object=answer_output)\n",
    "answer_prompt=PromptTemplate(input_variables=[\"question\",\"type\"],template=\"\"\"\n",
    "Answer for this question {question} which is of this type {type} and return in the below format\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "partial_variables={\"format_instructions\":answer_parser.get_format_instructions()})\n",
    "answer_chain=answer_prompt|llm|answer_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20776bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class summary_output(BaseModel):\n",
    "    question:str\n",
    "    summary:str\n",
    "\n",
    "summary_parser=PydanticOutputParser(pydantic_object=summary_output)\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": summary_parser.get_format_instructions()\n",
    "    },\n",
    "    template=\"\"\"\n",
    "You are a JSON generator.\n",
    "\n",
    "Summarize the following question and answer.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Rules:\n",
    "- Output ONLY valid JSON\n",
    "- No explanations\n",
    "- No extra text\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "summary_chain=summary_prompt|llm|summary_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d7025f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=(\n",
    "    validity_chain\n",
    "    | RunnableBranch(\n",
    "        (\n",
    "        lambda x:x.type==\"valid\",\n",
    "        RunnableLambda(lambda x:{\"question\":x.question})\n",
    "        |rewrite_chain\n",
    "        |RunnableLambda(lambda x:{\"question\":x.question})\n",
    "        |intent_chain\n",
    "        |RunnableBranch(\n",
    "        (\n",
    "        lambda x:x.intent!=\"opinion\",\n",
    "        RunnableLambda(lambda x:{\"question\":x.question,\"type\":x.intent})\n",
    "        |answer_chain\n",
    "        |RunnableLambda(lambda x:{\"question\":x.question,\"answer\":x.answer})\n",
    "        |summary_chain  \n",
    "        |RunnableLambda(lambda x:x.summary)\n",
    "        ),\n",
    "        (\n",
    "            RunnableLambda(lambda _:\"sorry i cant answer to this\")\n",
    "        )\n",
    "        ),\n",
    "        )\n",
    "        ,\n",
    "        (lambda x:x.type=='ambiguous',\n",
    "        RunnableLambda(lambda x:{\"question\":\"As of today\"+x.question})\n",
    "        |rewrite_chain\n",
    "        |RunnableLambda(lambda x:{\"question\":x.question})\n",
    "        |intent_chain\n",
    "        |RunnableBranch(\n",
    "            (\n",
    "            lambda x:x.intent!=\"opinion\",\n",
    "            RunnableLambda(lambda x:{\"question\":x.question,\"type\":x.intent})\n",
    "            |answer_chain\n",
    "            |RunnableLambda(lambda x:{\"question\":x.question,\"answer\":x.answer})\n",
    "            |summary_chain  \n",
    "            |RunnableLambda(lambda x:x.summary)\n",
    "            ),\n",
    "            (\n",
    "                RunnableLambda(lambda _:\"sorry i cant answer to this\")\n",
    "            )\n",
    "        ),\n",
    "        ),\n",
    "        RunnableLambda(lambda _:\"this is not valid,it will not be proceeded further\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80e84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argentina\n",
      "A perceptron is a type of feedforward neural network that uses a single layer of weights and biases to classify inputs into one of two categories.\n",
      "this is not valid,it will not be proceeded further\n",
      "History, Science, Technology, Literature, Geography, Health and Medicine, Entertainment, Business and Economics, Education\n",
      "Tungsten is the hardest element found on our planet, with a Mohs hardness of 8-9 and a Vickers hardness of 2000-3000.\n",
      "this is not valid,it will not be proceeded further\n",
      "this is not valid,it will not be proceeded further\n",
      "It's difficult to determine the world's best programmer as it's subjective and can vary depending on the criteria used to measure programming skills.\n",
      "Codeforces problems, algorithms, and data structures such as graph theory, dynamic programming, and greedy algorithms.\n",
      "Konstantin Chernikov (also known as pavelkun) is currently considered one of the best programmers based on Codeforces and other relevant sources.\n",
      "History, Science, Technology, Literature, Geography, Health and Medicine, Entertainment, Business and Economics, Education\n",
      "History, Science, Technology, Literature, Geography, Health and Medicine, Entertainment, Business and Economics, Education\n",
      "Clarity, Relevance, Specificity, Objectivity, Testability, Scope, Accuracy, Completeness, Consistency, Validity\n",
      "Clarity, Relevance, Specificity, Objectivity, Testability, Scope, Accuracy, Completeness, Consistency, Validity\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query=input(\"Enter your query: \").lower()\n",
    "    if query==\"exit\":\n",
    "        break\n",
    "    result=pipeline.invoke({\"question\":query})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4d2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
